import torch
import torch.nn as nn
import os
from transformers import AutoConfig, AutoTokenizer

# ç¡®ä¿è¿™é‡Œçš„ import è·¯å¾„å’Œä½ é¡¹ç›®é‡Œçš„æ–‡ä»¶ç»“æž„å¯¹åº”
# å‡è®¾ä½ çš„æ¨¡åž‹å®šä¹‰åœ¨ video_chatgpt_multimodal.py é‡Œ
from video_chatgpt_multimodal import MultimodalVideoChatGPTLlamaForCausalLM 

def safe_load_model(model_path, tokenizer_path="/userhome/cs/u3598820/base/base_model_videoChatGPT", pose_feature_dim=68, device='cuda'):
    """
    æ™ºèƒ½åŠ è½½æ¨¡åž‹ (Ultimate Version)
    
    åŠŸèƒ½:
    1. æ”¯æŒåˆ†ç‰‡æ¨¡åž‹ (Sharded Checkpoints): è‡ªåŠ¨è¯»å– model.safetensors.index.jsonã€‚
    2. è‡ªåŠ¨ç»´åº¦æ£€æŸ¥ (Dimension Check):
       - å¦‚æžœ Checkpoint é‡Œçš„ Projector ç»´åº¦åŒ¹é… (ä¾‹å¦‚æ˜¯ä»Ž Phase 1 æ¢å¤) -> å®Œç¾ŽåŠ è½½ã€‚
       - å¦‚æžœ Checkpoint é‡Œçš„ Projector ç»´åº¦ä¸åŒ¹é… (ä¾‹å¦‚æ˜¯åŽŸç‰ˆ X-VARS) -> è‡ªåŠ¨è·³è¿‡åŠ è½½è¯¥å±‚ï¼Œä½¿ç”¨å…¨æ–°çš„ nn.Linear éšæœºåˆå§‹åŒ–ã€‚
    
    Args:
        model_path: æ¨¡åž‹è·¯å¾„
        pose_feature_dim: ä½ çš„ Pose ç»´åº¦ (68)
        device: 'cuda' or 'cpu' or 'auto'
    """
    print(f"\n[SafeLoad] >>> Loading model from: {model_path}")
    print(f"[SafeLoad] >>> Target Pose Dim: {pose_feature_dim}")

    # 1. åŠ è½½ Config
    # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†èŽ·å–æ¨¡åž‹çš„å…ƒæ•°æ®ï¼Œç¡®ä¿é…ç½®æ­£ç¡®
    try:
        config = AutoConfig.from_pretrained(model_path)
    except Exception as e:
        raise ValueError(f"æ— æ³•ä»Ž {model_path} åŠ è½½ config.json. è¯·æ£€æŸ¥è·¯å¾„ã€‚é”™è¯¯: {e}")

    # 2. æ ¸å¿ƒåŠ è½½é€»è¾‘
    # æˆ‘ä»¬ä½¿ç”¨ ignore_mismatched_sizes=Trueï¼Œè¿™ä¸€ä¸ªå‚æ•°å°±å®žçŽ°äº†ä½ æ‰€æœ‰çš„éœ€æ±‚ï¼š
    # - ä¹Ÿå°±æ˜¯: "èƒ½åŒ¹é…å°±åŠ è½½ï¼Œä¸èƒ½åŒ¹é…å°±ä¿æŒéšæœºåˆå§‹åŒ–(New nn.Linear)"
    print("[SafeLoad] >>> Instantiating model...")
    
    try:
        model = MultimodalVideoChatGPTLlamaForCausalLM.from_pretrained(
            model_path,
            config=config,
            pose_feature_dim=pose_feature_dim, # ä¼ å…¥æ–°ç»´åº¦ï¼Œæ¨¡åž‹å†…éƒ¨ä¼šåˆ›å»º [4096, 1024+68] çš„å±‚
            # device_map=device,                 # è‡ªåŠ¨å¤„ç†æ˜¾å­˜åˆ†é…
            low_cpu_mem_usage=True,
            # torch_dtype=torch.float16,         # èŠ‚çœæ˜¾å­˜

            torch_dtype=torch.bfloat16,    # ðŸ”¥ æ”¹æˆ bfloat16

            ignore_mismatched_sizes=True       # ðŸ”¥ æ ¸å¿ƒï¼šå¦‚æžœç»´åº¦å¯¹ä¸ä¸Šï¼Œè‡ªåŠ¨ä¸¢å¼ƒæ—§æƒé‡ï¼Œä½¿ç”¨æ–°åˆå§‹åŒ–çš„å±‚
        )
    except OSError:
        # å¦‚æžœä¸æ˜¯åˆ†ç‰‡æ¨¡åž‹ï¼Œæˆ–è€…æ˜¯æ—§ç‰ˆæƒé‡çš„ç‰¹æ®Šæƒ…å†µï¼Œè¿™é‡Œå…œåº•æç¤º
        print("âŒ Load failed. ensure your model path contains .safetensors or .bin files.")
        raise







    # =================================================================
    # 3. ðŸ”¥ æ‰‹åŠ¨ä¿®å¤ Projector (Manual Fix for Meta Tensor) ðŸ”¥
    # =================================================================
    print(f"[SafeLoad] >>> Manually re-initializing mm_projector...")

    # A. èŽ·å–ç»´åº¦å‚æ•°
    # 1. LLM Hidden Size (é€šå¸¸æ˜¯ 4096)
    llm_hidden_size = config.hidden_size
    
    # 2. Vision Hidden Size (é€šå¸¸æ˜¯ 1024)
    # ä½ çš„ MultimodalVideoChatGPTLlamaModel ç±»é‡Œé»˜è®¤æ˜¯ 1024ï¼Œæˆ–è€…ä»Ž config è¯»å–
    mm_hidden_size = getattr(config, "mm_hidden_size", 1024)
    
    # 3. è®¡ç®—èžåˆåŽçš„è¾“å…¥ç»´åº¦
    # æ ¹æ®ä½ çš„ç±»å®šä¹‰: fused_input_dim = self.mm_hidden_size + self.pose_feature_dim
    fused_input_dim = mm_hidden_size + pose_feature_dim

    print(f"[SafeLoad] >>> Geometry: [CLIP({mm_hidden_size}) + Pose({pose_feature_dim})] -> {fused_input_dim} => LLM({llm_hidden_size})")

    # B. Create new Linear layer
    # Initialize directly to get into CPU memory, resolving the "No data" error
    # must change to **b**float16 to prevent fp16 precision overflow
    new_projector = nn.Linear(fused_input_dim, llm_hidden_size, bias=True)
    new_projector = new_projector.to(dtype=torch.bfloat16)
    
    # C. å¼ºåˆ¶æ›¿æ¢æ¨¡åž‹ä¸­çš„ Meta Layer
    # æ ¹æ® VideoChatGPTLlamaForCausalLM çš„ç»“æž„ï¼Œprojector ä½äºŽ model.model.mm_projector
    if hasattr(model.model, 'mm_projector'):
        model.model.mm_projector = new_projector
        torch.nn.init.normal_(model.model.mm_projector.weight, std=0.01) # ç¼©å°åˆå§‹æƒé‡çš„æ ‡å‡†å·®
        torch.nn.init.zeros_(model.model.mm_projector.bias)             # åç½®æ¸…é›¶
    else:
        # é˜²å¾¡æ€§ä»£ç ï¼šä¸‡ä¸€ç»“æž„å±‚çº§ä¸åŒ
        raise AttributeError(f"Critical: Could not find 'mm_projector' in model.model. Keys: {dir(model.model)}")

    print(f"[SafeLoad] >>> Projector successfully replaced with: {new_projector}")
    # =================================================================











    # 3. éªŒè¯ Projector çŠ¶æ€ (User Verification)
    # ä¸ºäº†è®©ä½ æ”¾å¿ƒï¼Œæˆ‘ä»¬æ‰“å°ä¸€ä¸‹ Projector çš„æƒé‡ä¿¡æ¯
    # æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹æƒé‡æ˜¯å¦å…¨ä¸º 0 æˆ–è€…æ˜¯å¦ç¬¦åˆç‰¹å®šåˆ†å¸ƒæ¥çŒœæµ‹ï¼ˆä½†é€šå¸¸æ²¡å¿…è¦ï¼‰
    # è¿™é‡Œæˆ‘ä»¬ä¸»è¦ç¡®è®¤å½¢çŠ¶æ˜¯å¯¹çš„
    current_shape = model.model.mm_projector.weight.shape
    print(f"[SafeLoad] >>> Model loaded. Current Projector Shape: {current_shape}")
    print(f"[SafeLoad] >>> (Expected: [Output_Dim, 1024 + {pose_feature_dim}])")

    # æ‰‹åŠ¨ç§»åŠ¨åˆ° GPU
    print(f"[SafeLoad] >>> Moving model to {device}...")
    model.to(device)

    # 4. åŠ è½½ Tokenizer
    print(f"[SafeLoad] >>> Loading Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)

    print("[SafeLoad] >>> âœ… Success.\n")
    return model, tokenizer