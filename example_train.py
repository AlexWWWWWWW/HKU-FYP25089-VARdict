import os
import torch
import glob
import transformers
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import TrainingArguments, Trainer, AutoConfig, AutoTokenizer
from load_model import safe_load_model

# è¯·ç¡®ä¿è¿™ä¸¤ä¸ª import è·¯å¾„æ­£ç¡®ï¼Œå¯¹åº”ä½ ä¹‹å‰ä¿å­˜çš„æ–‡ä»¶
from dataset import VARdictDataset 
from video_chatgpt_multimodal import MultimodalVideoChatGPTLlamaForCausalLM 

def get_latest_checkpoint(output_dir):
    """
    æ£€æŸ¥è¾“å‡ºç›®å½•ï¼Œå¯»æ‰¾æœ€æ–°çš„ checkpoint
    """
    if not os.path.exists(output_dir):
        return None
    # æŸ¥æ‰¾ checkpoint-* æ–‡ä»¶å¤¹
    checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
    if not checkpoints:
        return None
    # æŒ‰ä¿®æ”¹æ—¶é—´æŽ’åºï¼Œæ‰¾æœ€æ–°çš„
    latest_ckpt = max(checkpoints, key=os.path.getmtime)
    return latest_ckpt

def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
    )

def main():
    # ================= é…ç½®åŒºåŸŸ =================
    # X-VARS åŽŸå§‹æƒé‡è·¯å¾„ (Stage 1 Model)
    X_VARS_WEIGHTS = "/userhome/cs/u3598820/X-VARS_weight/X-VARS_weights" 
    
    # ä½ çš„æ•°æ®è·¯å¾„
    DATA_ROOT = "./mini_dataset"
    JSON_QA = "./annotations/annotations_train.json"
    
    # è¿™é‡Œçš„ JSON_PRED æ˜¯ Stage 1 åˆ†ç±»å™¨ç”Ÿæˆçš„é¢„æµ‹ç»“æžœ
    # å¿…é¡»å­˜åœ¨ï¼Œå¦åˆ™ dataset æž„å»º Prompt æ—¶ä¼šæŠ¥é”™
    JSON_PRED = "./predictionsTrain_clip.json" 
    
    OUTPUT_DIR = "./checkpoints_debug"
    
    # ================= 1. ç¡®å®šåŠ è½½è·¯å¾„ =================
    # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å·²ç»è®­ç»ƒè¿‡çš„ checkpoint
    latest_ckpt = get_latest_checkpoint(os.path.join(OUTPUT_DIR, "phase1")) # å…ˆçœ‹ Phase 1 è·‘å®Œæ²¡
    if not latest_ckpt:
        latest_ckpt = get_latest_checkpoint(OUTPUT_DIR) # æˆ–è€…æ ¹ç›®å½•
        
    if latest_ckpt:
        print(f">>> Found checkpoint: {latest_ckpt}. Resuming training...")
        load_path = latest_ckpt
    else:
        print(f">>> No local checkpoint found. Loading base X-VARS weights...")
        load_path = X_VARS_WEIGHTS

    # ================= 2. åŠ è½½æ¨¡åž‹ =================
    model, tokenizer = safe_load_model(
        load_path, 
        pose_feature_dim=68, # ä½ çš„ Pose ç»´åº¦
        device='cuda'
    )


    # ================= å¿…é¡»æ’å…¥çš„ä½œè€…é€»è¾‘ =================
    # è¿™ä¸€æ­¥ä¸æ˜¯ä¸ºäº†è®­ç»ƒå‚æ•°ï¼Œè€Œæ˜¯ä¸ºäº†æŠŠ Token ID (32003) å¡«å…¥ model.config
    # ä»Žè€Œä¿®å¤ forward é‡Œçš„æŠ¥é”™
    model.get_model().initialize_vision_modules(pretrain_mm_mlp_adapter=None)
    vision_config = model.get_model().vision_config
    
    # è¿™äº› Flag å†³å®šäº†æ•°æ®æµå‘
    model.config.tune_mm_mlp_adapter = True 
    model.config.freeze_mm_mlp_adapter = False
    model.config.mm_use_vid_start_end = True
    vision_config.use_vid_start_end = True
    model.config.sep_video_conv_front = False

    # ðŸ”¥ æ ¸å¿ƒï¼šæŠŠ Tokenizer é‡Œçš„ ID åŒæ­¥ç»™ Config ðŸ”¥
    model.initialize_vision_tokenizer(
        mm_use_vid_start_end=True, 
        tokenizer=tokenizer, 
        device='cuda', 
        tune_mm_mlp_adapter=False, 
        pretrain_mm_mlp_adapter=None
    )
    
    model.resize_token_embeddings(len(tokenizer))
    # ===================================================




    
    # å‡†å¤‡æ•°æ®é›†
    train_dataset = VARdictDataset(
        data_root=DATA_ROOT,
        split="Train",
        json_path_qa=JSON_QA,
        json_path_predictions=JSON_PRED,
        tokenizer=tokenizer
    )
    
    from transformers import DataCollatorForLanguageModeling
    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

    # ================= Phase 1: Train Projector Only =================
    # åªæœ‰å½“ä»Ž X-VARS åŠ è½½æ—¶ï¼Œæˆ–è€…æˆ‘ä»¬æ˜Žç¡®æƒ³å†è·‘ä¸€æ¬¡ Phase 1 æ—¶æ‰æ‰§è¡Œ
    # è¿™é‡Œç®€å•å¤„ç†ï¼šæ€»æ˜¯è·‘ä¸€æ¬¡ Phase 1ï¼Œå¦‚æžœæƒ³è·³è¿‡å¯ä»¥æ‰‹åŠ¨æ³¨é‡Š
    
    print("\n" + "="*20 + " Phase 1: Train Projector Only " + "="*20)
    


    # è®¾ç½®æ¢¯åº¦ï¼šå†»ç»“æ‰€æœ‰ï¼Œè§£å†» Projector
    for name, param in model.named_parameters():
        if "mm_projector" in name:
            param.requires_grad = True
        else:
            param.requires_grad = False
    
    print_trainable_parameters(model)
    
    args_phase1 = TrainingArguments(
        output_dir=os.path.join(OUTPUT_DIR, "phase1"),
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,

        gradient_checkpointing=True,

        # gradient_checkpointing=False, # æš‚æ—¶å…³æŽ‰ï¼ŒæŽ’é™¤æŽ‰å®ƒå’Œç¡¬ä»¶/é©±åŠ¨çš„å…¼å®¹æ€§é—®é¢˜
        # predict_with_generate=False,

        fp16=False,
        bf16=True,

        # max_grad_norm=1.0,  # ðŸ”¥ æ ¸å¿ƒé˜²å¾¡ï¼šæ¢¯åº¦è£å‰ª
        # æ­£ä¹‰å¿…èƒœ
        # victory for justice

        # learning_rate=1e-3, 
        learning_rate=1e-4, 
        num_train_epochs=1,
        save_strategy="epoch", # è·‘å®Œ Phase 1 å­˜ä¸€ä¸‹ï¼Œé˜²æ­¢å´©äº†é‡æ¥
        logging_steps=1,
        remove_unused_columns=False,
        report_to="none"
    )
    
    trainer1 = Trainer(
        model=model,
        args=args_phase1,
        train_dataset=train_dataset,
        data_collator=data_collator
    )
    
    trainer1.train()
    trainer1.save_model(os.path.join(OUTPUT_DIR, "phase1", "final"))

    # ================= Phase 2: Train LLM (LoRA) Only =================
    print("\n" + "="*20 + " Phase 2: Train LLM (LoRA) Only " + "="*20)
    
    # æ­¤æ—¶æ¨¡åž‹å·²ç»ç»è¿‡äº† Phase 1 çš„è®­ç»ƒï¼ˆå†…å­˜ä¸­å·²ç»æ˜¯æ–°æƒé‡ï¼‰
    # æ·»åŠ  LoRA Config
    
    model.config.tune_mm_mlp_adapter = False 
    model.config.freeze_mm_mlp_adapter = True


    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    # æ³¨æ„ï¼šget_peft_model ä¼šæŠŠåŽŸæ¨¡åž‹åŒ…ä¸€å±‚ã€‚
    # å¦‚æžœåŠ è½½çš„æ¨¡åž‹å·²ç»æ˜¯ PeftModel (æ¯”å¦‚ Resume ä¸” Phase 2 è·‘äº†ä¸€åŠ)ï¼Œè¿™é‡Œéœ€è¦ç‰¹æ®Šå¤„ç†
    # ä¸ºäº† MVP ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾ Phase 1 äº§å‡ºçš„æ˜¯ Base Model + Updated Projector
    model = get_peft_model(model, peft_config)
    
    # è®¾ç½®æ¢¯åº¦ï¼šå†»ç»“ Projectorï¼Œè§£å†» LoRA
    for name, param in model.named_parameters():
        if "mm_projector" in name:
            param.requires_grad = False 
        elif "lora_" in name:           
            param.requires_grad = True
        else:
            param.requires_grad = False 
            
    print_trainable_parameters(model)
    
    args_phase2 = TrainingArguments(
        output_dir=os.path.join(OUTPUT_DIR, "phase2"),
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=2e-4, 
        num_train_epochs=1,
        save_strategy="no", # æœ€åŽä¸å­˜ï¼Œä¿æŒç¡¬ç›˜æ¸…æ´ (æŒ‰ä½ è¦æ±‚)
        logging_steps=1,
        remove_unused_columns=False,
        report_to="none",


        gradient_checkpointing=True,
        fp16=False,
        bf16=True,

    )
    
    trainer2 = Trainer(
        model=model,
        args=args_phase2,
        train_dataset=train_dataset,
        data_collator=data_collator
    )
    
    trainer2.train()
    
    trainer2.save_model(os.path.join(OUTPUT_DIR, "phase2", "final_lora"))

    print("\n>>> All training finished!")

if __name__ == "__main__":
    main()