import os
import pickle
import numpy as np
import torch
from torch.utils.data import Dataset
import json
import glob
import sys

# è¯·ç¡®ä¿è¿™ä¸ªè·¯å¾„åŒ…å« video_chatgpt æ–‡ä»¶å¤¹
# sys.path.append("/userhome/cs/u3598820/X-VARS/X-VARS")

from video_chatgpt.video_conversation import conv_templates, SeparatorStyle

# ================= å¸¸é‡å®šä¹‰ =================
IGNORE_INDEX = -100
DEFAULT_VIDEO_PATCH_TOKEN = "<vid_patch>"
DEFAULT_VID_START_TOKEN = "<vid_start>"
DEFAULT_VID_END_TOKEN = "<vid_end>"

class VARdictDataset(Dataset):
    """
    VARdict å¤šæ¨¡æ€æ•°æ®é›† (Optimized Version)
    - æ ¸å¿ƒä¼˜åŒ–: Prediction Key ç›´æ¥åŒ¹é…ï¼Œç§»é™¤ O(N) å¾ªç¯
    """
    def __init__(self, 
                 data_root, 
                 split, 
                 json_path_qa,           # annotations_train.json
                 json_path_predictions,  # clean åçš„ predictionsTrain_clip.json
                 tokenizer, 
                 video_token_len=300):
        
        self.data_root = os.path.join(data_root, split)
        self.split = split
        self.tokenizer = tokenizer
        self.video_token_len = video_token_len
        self.conv_mode = "video-chatgpt_v1"
        
        # ---------------------------------------------------------
        # 1. åŠ è½½ QA æ ‡æ³¨
        # ---------------------------------------------------------
        print(f"Loading annotations from {json_path_qa}...")
        with open(json_path_qa, 'r') as f:
            raw_data = json.load(f)
            # å»ºç«‹æŸ¥æ‰¾è¡¨: "action_0" -> {question, answer}
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ annotations é‡Œçš„ path å°±æ˜¯ "action_0" è¿™ç§æ ¼å¼
            self.qa_lookup = {item['path']: item for item in raw_data}

        # ---------------------------------------------------------
        # 2. åŠ è½½ Prediction (ä½ çš„ Clean JSON)
        # ---------------------------------------------------------
        print(f"Loading predictions from {json_path_predictions}...")
        with open(json_path_predictions, 'r') as f2:
            pred_data = json.load(f2)
            # å¤„ç†ä½ çš„ JSON ç»“æ„: {"Set": "Train", "Actions": {...}}
            if "Actions" in pred_data:
                self.pred = pred_data["Actions"]
            else:
                self.pred = pred_data
        
        # æ‰“å°ä¸€ä¸ª Key æ ·ä¾‹ç”¨äºè°ƒè¯•ï¼Œç¡®ä¿åŠ è½½æ­£ç¡®
        if len(self.pred) > 0:
            example_key = list(self.pred.keys())[0]
            print(f"Prediction Key Example: '{example_key}'")

        # ---------------------------------------------------------
        # 3. æ‰«ææ–‡ä»¶ç³»ç»Ÿå¹¶æ„å»º Sample List
        # ---------------------------------------------------------
        self.samples = []
        print(f"Scanning files in {self.data_root}...")
        
        action_dirs = glob.glob(os.path.join(self.data_root, "action_*"))
        
        for action_dir in action_dirs:
            # action_dir_name: "action_0"
            action_dir_name = os.path.basename(action_dir)
            
            # æ£€æŸ¥ QA ä¸­æ˜¯å¦æœ‰æ­¤ action
            # å…¼å®¹æ€§å¤„ç†ï¼šæœ‰äº› QA json é‡Œçš„ path å¯èƒ½æ˜¯ "Train/action_0"
            # è¿™é‡Œä¼˜å…ˆåŒ¹é… "action_0"
            qa_key = action_dir_name
            if qa_key not in self.qa_lookup:
                # å°è¯•åŠ ä¸Š split å†æ¬¡æŸ¥æ‰¾ (é˜²æ­¢ QA key æ˜¯ Train/action_0)
                alt_key = f"{self.split}/{action_dir_name}"
                if alt_key in self.qa_lookup:
                    qa_key = alt_key
                else:
                    # ç¡®å®æ‰¾ä¸åˆ°ï¼Œè·³è¿‡
                    continue

            # æ‰«æ .pkl æ–‡ä»¶
            pkl_files = glob.glob(os.path.join(action_dir, "PRE_CLIP_feature_clip_*.pkl"))
            
            for pkl_path in pkl_files:
                filename = os.path.basename(pkl_path) 
                # filename e.g.: "PRE_CLIP_feature_clip_1.pkl"
                
                # æå– clip_id e.g.: "clip_1"
                clip_id = filename.replace("PRE_CLIP_feature_", "").replace(".pkl", "")
                
                # æ¨æ–­ Pose è·¯å¾„
                dirname = os.path.dirname(pkl_path)
                npy_path = os.path.join(dirname, f"{clip_id}_pose.npy")
                
                # åªæœ‰ CLIP å’Œ Pose éƒ½å­˜åœ¨æ‰ç®—æœ‰æ•ˆæ•°æ®
                if os.path.exists(npy_path):
                    
                    # ğŸ”¥ å…³é”®ç‚¹ï¼šæ„é€  Prediction Lookup Key ğŸ”¥
                    # æ ¹æ®ä½ æä¾›çš„ JSONï¼ŒKey æ˜¯ "action_0/PRE_CLIP_feature_clip_1.pkl"
                    # ä¹Ÿå°±æ˜¯: action_dir_name / filename
                    pred_key = f"{action_dir_name}/{filename}"

                    self.samples.append({
                        'clip_path': pkl_path,
                        'pose_path': npy_path,
                        'action_key': qa_key,      # ç”¨äºæŸ¥ QA
                        'pred_key': pred_key,      # ç”¨äºæŸ¥ Prediction (ç›´æ¥åŒ¹é…ï¼Œæ— éœ€å¾ªç¯)
                        'debug_id': f"{action_dir_name}/{clip_id}"
                    })
        
        print(f"[{split}] Loaded {len(self.samples)} valid samples.")

    def preprocess_text(self, qa_key, pred_key):
        """
        æ„å»º Prompt
        - qa_key: ç”¨äº self.qa_lookup
        - pred_key: ç”¨äº self.pred (ç²¾å‡†åŒ¹é…)
        """
        # 1. è·å– QA
        qa_data = self.qa_lookup[qa_key]
        question = qa_data["question"]
        answer = qa_data["answer"]

        # 2. è·å– Prediction (O(1) æŸ¥æ‰¾)
        # é»˜è®¤å€¼
        pred_action = "unknown"
        pred_off = "unknown"
        pred_card = "unknown"

        if pred_key in self.pred:
            pred_entry = self.pred[pred_key]
            pred_action = pred_entry.get("Action class", "unknown")
            pred_off = pred_entry.get("Offence", "unknown")
            pred_card = str(pred_entry.get("Severity", "unknown")) # è½¬å­—ç¬¦ä¸²é˜²æ­¢ float æŠ¥é”™
        else:
            # è¿™ç§æƒ…å†µç†è®ºä¸Šæå°‘å‘ç”Ÿï¼ˆé™¤é JSON å’Œæ–‡ä»¶ç³»ç»Ÿä¸ä¸€è‡´ï¼‰
            # print(f"Warning: Key {pred_key} not found in predictions.")
            pass

        # 3. æ ¼å¼åŒ– Prediction æ–‡æœ¬ (VARS é€»è¾‘)
        if pred_off == "Offence":
            pred_off = ", foul and "
        elif pred_off == "No offence":
            pred_off = "and no foul."
        
        if pred_card == "1.0" or pred_card == "1":
            pred_off += "no card."
        elif pred_card == "3.0" or pred_card == "3":
            pred_off += "a yellow card."
        elif pred_card == "5.0" or pred_card == "5":
            pred_off += "a red card."

        action_map = {
            "Tackling": "a tackle ",
            "Standing tackling": "a foot duel ",
            "Elbowing": "using his elbows or arms ",
            "Holding": "holding ",
            "High leg": "a high leg ",
            "Pushing": "pushing ",
            "Challenge": "a shoulder challenge ",
            "Dive": "a simulation "
        }
        pred_action = action_map.get(pred_action, pred_action + " ")

        # 4. ç»„è£… Prompt
        qs = question + " The prediction for this video is " + pred_action + pred_off + '\n' + DEFAULT_VID_START_TOKEN + DEFAULT_VIDEO_PATCH_TOKEN * self.video_token_len + DEFAULT_VID_END_TOKEN
        
        conv = conv_templates[self.conv_mode].copy()
        conv.append_message(conv.roles[0], qs)
        conv.append_message(conv.roles[1], answer)
        prompt = conv.get_prompt()

        # 5. Tokenize
        input_ids = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding="max_length",
            max_length=self.tokenizer.model_max_length,
            truncation=True,
        ).input_ids

        targets = input_ids.clone()

        # 6. Masking (åªè®­ç»ƒ Assistant å›ç­”éƒ¨åˆ†)
        sep = "ASSISTANT:"
        total_len = int(targets.ne(self.tokenizer.pad_token_id).sum())
        
        parts = prompt.split(sep)
        if len(parts) >= 2:
            # Mask æ‰ "USER: ... \nASSISTANT:"
            instruction_len = len(self.tokenizer(parts[0]).input_ids) - 1 
            targets[0, :instruction_len] = IGNORE_INDEX
        
        # Mask æ‰ Padding
        cur_len = total_len
        targets[0, cur_len:] = IGNORE_INDEX

        return dict(
            input_ids=input_ids.squeeze(),
            labels=targets.squeeze(),
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id).squeeze(),
        )

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. Load CLIP
        try:
            with open(sample['clip_path'], 'rb') as f:
                clip_features = pickle.load(f)
            clip_tensor = torch.from_numpy(clip_features).float()
        except Exception as e:
            print(f"Error loading CLIP {sample['clip_path']}: {e}")
            clip_tensor = torch.zeros(self.video_token_len, 1024).float()
        
        # print(clip_tensor.shape)

        # --- CLIP å½’ä¸€åŒ–æµ‹è¯•ï¼šå¼€å§‹ ---
        # print(f"DEBUG [Idx:{idx}] | CLIP Before Norm: Min={clip_tensor.min():.2e}, Max={clip_tensor.max():.2e}")
        
        # ä½¿ç”¨ L2 å½’ä¸€åŒ–ï¼šå°†ç‰¹å¾å‘é‡æ˜ å°„åˆ°å•ä½çƒé¢ä¸Šï¼Œå½»åº•å¹²æ‰ 122 è¿™ç§ç¦»ç¾¤å€¼
        if clip_tensor.numel() > 0:
            clip_tensor = torch.nn.functional.normalize(clip_tensor, p=2, dim=-1)
        
        # print(f"DEBUG [Idx:{idx}] | CLIP After Norm:  Min={clip_tensor.min():.2e}, Max={clip_tensor.max():.2e}")
        # --- CLIP å½’ä¸€åŒ–æµ‹è¯•ï¼šç»“æŸ ---
        

        # 2. Load Pose
        try:
            pose_data = np.load(sample['pose_path'])
            if pose_data.shape[0] > 0:
                pose_flat = pose_data.reshape(pose_data.shape[0], -1)
            else:
                pose_flat = np.zeros((1, 68))
            pose_tensor = torch.from_numpy(pose_flat).float()
        except Exception as e:
            print(f"Error loading Pose {sample['pose_path']}: {e}")
            pose_tensor = torch.zeros(1, 68).float()
        # print(pose_tensor.shape)

        # --- æ‰“å°æµ‹è¯•ï¼šå½’ä¸€åŒ–å‰ ---
        # print(f"DEBUG [Idx:{idx}] | Pose Before Norm: Min={pose_tensor.min():.2f}, Max={pose_tensor.max():.2f}")
        
        # normalization
        if pose_tensor.numel() > 0: pose_tensor = pose_tensor / (pose_tensor.abs().max() + 1e-6)

        # --- æ‰“å°æµ‹è¯•ï¼šå½’ä¸€åŒ–å ---
        # print(f"DEBUG [Idx:{idx}] | Pose After Norm:  Min={pose_tensor.min():.2f}, Max={pose_tensor.max():.2f}")


        # ================= ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¼ºåˆ¶æ’å€¼å¯¹é½åˆ° 300 ğŸ”¥ =================
        # 1. è°ƒæ•´ç»´åº¦é€‚åº” interpolate: [T, 68] -> [1, 68, T]
        pose_tensor = pose_tensor.permute(1, 0).unsqueeze(0)
        
        # 2. æ’å€¼: æ— è®ºåŸæ¥å¤šé•¿ï¼Œç»Ÿç»Ÿå˜æˆ 300 (ä¸ CLIP é•¿åº¦ä¸€è‡´)
        pose_tensor = torch.nn.functional.interpolate(
            pose_tensor, 
            size=clip_tensor.shape[-2],
            mode='linear', 
            align_corners=False
        )
        
        # 3. è¿˜åŸç»´åº¦: [1, 68, 300] -> [300, 68]
        pose_tensor = pose_tensor.squeeze(0).permute(1, 0)
        # ===================================================================

        # print(pose_tensor.shape) # æ­¤æ—¶æ°¸è¿œæ˜¯ [300, 68]



        # 3. Process Text (ç›´æ¥ä¼ å…¥å‡†å¤‡å¥½çš„ key)
        text_data = self.preprocess_text(sample['action_key'], sample['pred_key'])

        # åœ¨ __getitem__ æœ€å
        valid_labels = (text_data["labels"] != -100).sum()
        if valid_labels == 0:
            print(f"âš ï¸ Warning: Sample {idx} has NO valid labels (all -100)!")


        # print("LABELS:", text_data["labels"], "===========")

        # ================= ğŸ”¥ æ ¸å¿ƒç›‘æ§ï¼šLabel æœ‰æ•ˆæ€§æ£€æŸ¥ ğŸ”¥ =================
        labels_tensor = text_data["labels"] # å‡è®¾å®ƒæ˜¯ tensor
        
        # 1. è®¡ç®—ç†è®ºä¸Šâ€œå…¨è™šæ— â€æ—¶çš„æ€»å’Œ
        # å¦‚æœå…¨æ˜¯ -100ï¼Œsum åº”è¯¥ç­‰äºï¼šé•¿åº¦ * -100
        num_elements = labels_tensor.numel()
        expected_void_sum = num_elements * -100
        
        # 2. è®¡ç®—å®é™…æ€»å’Œä¸æœ‰æ•ˆ Token æ•°é‡
        actual_sum = labels_tensor.sum().item()
        valid_label_mask = (labels_tensor != -100)
        num_valid_tokens = valid_label_mask.sum().item()
        
        # 3. æ‰“å°è¯Šæ–­ç»“æœ
        if num_valid_tokens == 0:
            print(f"âŒ [Idx:{idx}] !!! CRITICAL !!! ALL labels are -100. Expected Sum: {expected_void_sum}, Actual: {actual_sum}")
        else:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆ token çš„å€¼ä½œä¸ºå‚è€ƒ
            first_valid_val = labels_tensor[valid_label_mask][0].item()
            # print(f"âœ… [Idx:{idx}] Valid Labels Found! Count: {num_valid_tokens}/{num_elements}, First Valid TokenID: {first_valid_val}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éæ³• Token ID (æ¯”å¦‚è¶…å‡ºè¯è¡¨æˆ–è´Ÿæ•°)
        if num_valid_tokens > 0:
            max_id = labels_tensor.max().item()
            if max_id > 32000: # å‡è®¾ Llama è¯è¡¨ 32000 å·¦å³
                 print(f"âš ï¸ [Idx:{idx}] Warning: Token ID {max_id} might be out of vocab!")
        # ===================================================================


        return {
            "input_ids": text_data["input_ids"],
            "labels": text_data["labels"],
            "attention_mask": text_data["attention_mask"],
            "video_spatio_temporal_features": clip_tensor,
            "pose_spatio_temporal_features": pose_tensor
        }


# ==============================================================================
#  æµ‹è¯•æ¨¡å— (If Name == Main)
# ==============================================================================
if __name__ == "__main__":
    from transformers import AutoTokenizer

    # --- é…ç½® ---
    TEST_DATA_ROOT = "/userhome/cs/u3598820/HKU-FYP25089-VARdict/mini_dataset"
    TEST_JSON_QA = "/userhome/cs/u3598820/annotations/annotations_train.json"
    TEST_JSON_PRED = "/userhome/cs/u3598820/HKU-FYP25089-VARdict/predictionsTrain_clip.json"
    MODEL_PATH = "lmsys/vicuna-7b-v1.5" # æˆ–è€…æœ¬åœ°è·¯å¾„

    print("=== Starting Dataset Verification ===")

    # 1. Load Tokenizer
    try:
        print(f"Loading tokenizer from {MODEL_PATH}...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)
        tokenizer.add_special_tokens({'additional_special_tokens': ['<vid_start>', '<vid_end>', '<vid_patch>']})
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.unk_token
    except Exception as e:
        print(f"Tokenizer error: {e}. Using CLIP tokenizer as fallback.")
        from transformers import CLIPTokenizer
        tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
        tokenizer.add_special_tokens({'additional_special_tokens': ['<vid_start>', '<vid_end>', '<vid_patch>']})
        tokenizer.pad_token = tokenizer.eos_token

    # 2. Init Dataset
    try:
        dataset = VARdictDataset(
            data_root=TEST_DATA_ROOT,
            split="Train",
            json_path_qa=TEST_JSON_QA,
            json_path_predictions=TEST_JSON_PRED,
            tokenizer=tokenizer
        )
    except Exception as e:
        print(f"Dataset Init Failed: {e}")
        exit()

    # 3. Test __getitem__ and Content
    if len(dataset) > 0:
        print(f"\nFetching sample [0] (Key: {dataset.samples[0]['pred_key']})...")
        sample = dataset[0]

        # æ£€æŸ¥ Input IDs è§£ç 
        input_ids = sample['input_ids']
        valid_ids = input_ids.clone()
        valid_ids[valid_ids == -100] = tokenizer.pad_token_id
        decoded = tokenizer.decode(valid_ids, skip_special_tokens=False)

        print("-" * 40)
        print("DECODED PROMPT SNIPPET:")
        print(decoded[:800] + " ...")
        # print(decoded + " ...")
        print("-" * 40)

        if "The prediction for this video is" in decoded and "unknown" not in decoded:
            print("âœ… SUCCESS: Prediction injected correctly!")
        elif "unknown" in decoded:
            print("âš ï¸ WARNING: Prediction injected but values are 'unknown'. Check JSON key matching.")
        else:
            print("âŒ FAILURE: Prediction template missing.")
    else:
        print("âŒ Dataset empty. Check paths.")