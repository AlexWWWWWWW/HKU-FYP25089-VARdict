from typing import List, Optional, Tuple, Union
import torch
import torch.nn as nn
from torch.nn import CrossEntropyLoss
from transformers import AutoConfig, AutoModelForCausalLM, LlamaConfig, LlamaModel, LlamaForCausalLM
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast

# å¯¼å…¥åŸæœ‰çš„é…ç½®ç±»å’Œ Token å®šä¹‰
from video_chatgpt.model.video_chatgpt import (
    VideoChatGPTLlamaModel,
    VideoChatGPTLlamaForCausalLM,
    VideoChatGPTConfig,
    DEFAULT_VID_START_TOKEN,
    DEFAULT_VID_END_TOKEN,
    DEFAULT_VIDEO_PATCH_TOKEN
)

class MultimodalVideoChatGPTLlamaModel(VideoChatGPTLlamaModel):
    """
    æ‰©å±•çš„ VideoChatGPT æ¨¡å‹æ ¸å¿ƒï¼Œæ”¯æŒ CLIP + Pose (Concatenate èåˆ)
    """
    def __init__(
        self,
        config: LlamaConfig,
        mm_vision_tower=None,
        mm_hidden_size=None,
        pose_feature_dim: int = 68,  # é»˜è®¤ä¸º 68 (2äºº x 17ç‚¹ x 2åæ ‡)
    ):
        super(MultimodalVideoChatGPTLlamaModel, self).__init__(config, mm_vision_tower, mm_hidden_size)
        
        self.pose_feature_dim = pose_feature_dim
        
        # 1. è·å–åŸºç¡€é…ç½®
        if hasattr(config, "mm_hidden_size"):
            self.mm_hidden_size = config.mm_hidden_size
        else:
            self.mm_hidden_size = 1024 # é»˜è®¤ CLIP Large ç»´åº¦

        # 2. è®¡ç®—èåˆåçš„è¾“å…¥ç»´åº¦ (CLIP + Pose)
        # ä¾‹å¦‚: 1024 + 68 = 1092
        fused_input_dim = self.mm_hidden_size + self.pose_feature_dim
        
        # 3. é‡å†™æŠ•å½±å±‚ (Projector)
        # è¿™ä¸€å±‚å°†è´Ÿè´£æŠŠ (è§†é¢‘+åŠ¨ä½œ) çš„è”åˆç‰¹å¾æ˜ å°„åˆ° LLM çš„ 4096 ç»´ç©ºé—´
        # æ³¨æ„: åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œè¿™ä¸€å±‚ä¼šå› ä¸ºå½¢çŠ¶ä¸åŒ¹é…è€Œè¢«è·³è¿‡(éšæœºåˆå§‹åŒ–)ï¼Œè¿™æ˜¯ç¬¦åˆé¢„æœŸçš„ (Stage 2 éœ€è¦é‡è®­)
        if hasattr(config, "use_mm_proj") and config.use_mm_proj:
            self.mm_projector = nn.Linear(fused_input_dim, config.hidden_size)

    def fuse_features(self, clip_features: torch.Tensor, pose_features: torch.Tensor) -> torch.Tensor:
        """
        æ‰§è¡Œç‰¹å¾æ‹¼æ¥èåˆ
        """
        # A. å®‰å…¨æ£€æŸ¥ï¼šå°† Pose ç§»åŠ¨åˆ°ä¸ CLIP ç›¸åŒçš„è®¾å¤‡å’Œæ•°æ®ç±»å‹
        # CLIP é€šå¸¸æ˜¯ fp16/bf16 ä¸”åœ¨ GPU ä¸Šï¼ŒPose åˆšåŠ è½½å¯èƒ½æ˜¯ fp32/CPU
        if pose_features.device != clip_features.device:
            pose_features = pose_features.to(clip_features.device)
        if pose_features.dtype != clip_features.dtype:
            pose_features = pose_features.to(clip_features.dtype)

        # B. Align input length (Min-Pooling)
        # align according to the minimum length
        # min_len = min(clip_features.shape[1], pose_features.shape[1])
        # clip_features = clip_features[:, :min_len, :]
        # pose_features = pose_features[:, :min_len, :]
        # print(clip_features.shape, pose_features.shape)
        # C. Concatenation
        # [Batch, T, 1024] + [Batch, T, 68] -> [Batch, T, 1092]

        # æ£€æŸ¥æ˜¯å¦æœ‰ NaN æˆ– Inf
        if torch.isnan(clip_features).any() or torch.isinf(clip_features).any():
            print("âŒ CLIP Features contain NaN/Inf!")

        if torch.isnan(pose_features).any() or torch.isinf(pose_features).any():
            print("âŒ Pose Features contain NaN/Inf!")


        # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯ 0 (æœ€é‡è¦ï¼)
        if clip_features.sum() == 0 or pose_features.sum() == 0:
            print("âŒ CLIP/Pose Features are ALL ZEROS! (Data loading failed)")

        fused = torch.cat([clip_features, pose_features], dim=-1)
        

        # print(f"DEBUG | Fused Features - Min: {fused.min().item():.4e}, Max: {fused.max().item():.4e}, Mean: {fused.mean().item():.4e}")
        if torch.isnan(fused).any():
            print("âŒ Fused features contain NaN before projector!")
        return fused

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        video_spatio_temporal_features: Optional[torch.FloatTensor] = None,
        pose_spatio_temporal_features: Optional[torch.FloatTensor] = None, # æ–°å¢è¾“å…¥
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        

        # weight_sum = self.mm_projector.weight.sum().item()
        # print(f"DEBUG | Projector Weight Sum: {weight_sum}")
        
        if torch.isnan(self.mm_projector.weight).any():
             print("âŒ FATAL: Projector weight is NaN at the VERY START of forward!")

        

        if (input_ids < 0).any() or (input_ids >= self.config.vocab_size).any():
            print(f"âŒ CRITICAL: Illegal input_ids detected! Range: {input_ids.min()} to {input_ids.max()}")
        
        # å¤ç”¨çˆ¶ç±»çš„ embed_tokens
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        # --- å¤šæ¨¡æ€å¤„ç†é€»è¾‘ ---
        # åªæœ‰å½“å­˜åœ¨è§†é¢‘ç‰¹å¾ä¸” input_ids ä¸æ˜¯å•æ­¥ç”Ÿæˆ(shape!=1)æˆ–è€…æ˜¯è®­ç»ƒæ¨¡å¼æ—¶æ‰æ‰§è¡Œ
        if (input_ids.shape[1] != 1 or self.training) and video_spatio_temporal_features is not None:
            

            # ğŸ”¥ğŸ”¥ğŸ”¥ã€æ–°å¢ä¿®å¤ä»£ç ã€‘å¼€å§‹ ğŸ”¥ğŸ”¥ğŸ”¥
            # è·å– Projector çš„ç›®æ ‡ç²¾åº¦ (é€šå¸¸æ˜¯ float16)
            # target_dtype = self.mm_projector.weight.dtype
            # target_dtype = torch.float16
            target_dtype = torch.bfloat16
            
            # å¼ºåˆ¶å°†è¾“å…¥ç‰¹å¾è½¬ä¸ºç›®æ ‡ç²¾åº¦
            if video_spatio_temporal_features.dtype != target_dtype:
                video_spatio_temporal_features = video_spatio_temporal_features.to(target_dtype)
                
            if pose_spatio_temporal_features is not None:
                if pose_spatio_temporal_features.dtype != target_dtype:
                    pose_spatio_temporal_features = pose_spatio_temporal_features.to(target_dtype)
            # ğŸ”¥ğŸ”¥ğŸ”¥ã€æ–°å¢ä¿®å¤ä»£ç ã€‘ç»“æŸ ğŸ”¥ğŸ”¥ğŸ”¥


            # 1. ç‰¹å¾èåˆ
            if pose_spatio_temporal_features is not None:
                # æ­£å¸¸æƒ…å†µï¼šä¸¤ä¸ªç‰¹å¾éƒ½æœ‰
                fused_features = self.fuse_features(video_spatio_temporal_features, pose_spatio_temporal_features)
            else:
                # Fallback: å¦‚æœåªæœ‰è§†é¢‘æ²¡æœ‰ Pose (ä¾‹å¦‚æ¨ç†æ—¶æ•°æ®ç¼ºå¤±)
                # åˆ›å»ºå…¨ 0 çš„ Dummy Pose è¿›è¡Œæ‹¼æ¥ï¼Œä¿è¯ç»´åº¦èƒ½é€šè¿‡ Projector
                B, T, _ = video_spatio_temporal_features.shape
                dummy_pose = torch.zeros(
                    B, T, self.pose_feature_dim,
                    device=video_spatio_temporal_features.device,
                    dtype=video_spatio_temporal_features.dtype
                )
                fused_features = torch.cat([video_spatio_temporal_features, dummy_pose], dim=-1)

            # 2. æŠ•å½±åˆ° LLM ç©ºé—´ [Batch, T, 1092] -> [Batch, T, 4096]
            video_features = self.mm_projector(fused_features)

            # 3. åˆ›å»º Dummy ç‰¹å¾ (ç”¨äºå¡«å……éè§†é¢‘ Token ä½ç½®)
            # è¿™é‡Œçš„ç»´åº¦å¿…é¡»åŒ¹é…èåˆåçš„ç»´åº¦ (1092)
            dummy_video_features = torch.zeros(
                video_features.shape[1],
                self.mm_hidden_size + self.pose_feature_dim,
                device=inputs_embeds.device,
                dtype=inputs_embeds.dtype,
            )
            dummy_video_features = self.mm_projector(dummy_video_features)

            # 4. å°†è§†é¢‘ç‰¹å¾æ’å…¥åˆ° inputs_embeds ä¸­
            new_input_embeds = []
            cur_video_idx = 0
            for cur_input_ids, cur_input_embeds in zip(input_ids, inputs_embeds):
                # æƒ…å†µ A: è¿™ä¸€æ¡æ•°æ®é‡Œæ²¡æœ‰è§†é¢‘ token (çº¯æ–‡æœ¬)
                if (cur_input_ids == self.vision_config.vid_patch_token).sum() == 0:
                    # åŠ å…¥ dummy æ¢¯åº¦ä»¥é˜²æŠ¥é”™
                    cur_input_embeds = cur_input_embeds + (0. * dummy_video_features).sum()
                    new_input_embeds.append(cur_input_embeds)
                    cur_video_idx += 1
                    continue

                # æƒ…å†µ B: ä½¿ç”¨ <vid_start> å’Œ <vid_end> åŒ…è£¹è§†é¢‘
                if self.vision_config.use_vid_start_end:
                    if (cur_input_ids == self.vision_config.vid_start_token).sum() != \
                       (cur_input_ids == self.vision_config.vid_end_token).sum():
                        raise ValueError("The number of video start tokens and video end tokens should be the same.")
                    
                    video_start_tokens = torch.where(cur_input_ids == self.vision_config.vid_start_token)[0]
                    for video_start_token_pos in video_start_tokens:
                        cur_video_features = video_features[cur_video_idx].to(device=cur_input_embeds.device)
                        
                        # èåˆåçš„ç‰¹å¾é•¿åº¦ (min_len)
                        num_patches = cur_video_features.shape[0] 
                        
                        # æ‹¼æ¥: [Start Token] + [Video Features] + [End Token]
                        # è¿™é‡Œçš„åˆ‡ç‰‡é€»è¾‘å‡è®¾ Dataset ä¸­çš„å ä½ç¬¦é•¿åº¦è¶³ä»¥å®¹çº³ num_patches
                        # å®é™…ä¸Š VideoChatGPT é€šå¸¸åœ¨ Dataset é¢„å¤„ç†æ—¶å°±å¯¹é½äº†é•¿åº¦
                        cur_new_input_embeds = torch.cat((
                            cur_input_embeds[:video_start_token_pos + 1],
                            cur_video_features,
                            cur_input_embeds[video_start_token_pos + num_patches + 1:]
                        ), dim=0)
                        
                        cur_video_idx += 1
                    new_input_embeds.append(cur_new_input_embeds)
                
                # æƒ…å†µ C: ä¸ä½¿ç”¨ Start/End Token (ç›´æ¥æ›¿æ¢ Patch Token)
                else:
                    cur_video_features = video_features[cur_video_idx]
                    num_patches = cur_video_features.shape[0]
                    
                    if (cur_input_ids == self.vision_config.vid_patch_token).sum() != num_patches:
                         # è¿™é‡Œåšä¸ªå…¼å®¹ï¼šå¦‚æœ Token æ•°é‡ä¸åŒ¹é…ï¼Œå°è¯•æˆªæ–­æˆ–æŠ¥é”™
                         # ä¸ºäº†ç¨³å¥ï¼Œæˆ‘ä»¬ä»¥ç‰¹å¾é•¿åº¦ä¸ºå‡†è¿›è¡Œæ›¿æ¢
                         pass

                    masked_indices = torch.where(cur_input_ids == self.vision_config.vid_patch_token)[0]
                    mask_index_start = masked_indices[0]
                    
                    cur_new_input_embeds = torch.cat((
                        cur_input_embeds[:mask_index_start],
                        cur_video_features,
                        cur_input_embeds[mask_index_start + num_patches:]
                    ), dim=0)
                    new_input_embeds.append(cur_new_input_embeds)
                    cur_video_idx += 1

            inputs_embeds = torch.stack(new_input_embeds, dim=0)




        # æ£€æŸ¥æ¯ä¸€ä¸ªç¯èŠ‚
        # print(f"1. Inputs Embeds NaN: {torch.isnan(inputs_embeds).any()}")

        # if video_spatio_temporal_features is not None:
        #     print(f"2. Video Feats NaN: {torch.isnan(video_spatio_temporal_features).any()}")
        #     # æ£€æŸ¥ Projector åçš„è¾“å‡º
        #     video_features_projected = self.mm_projector(fused_features)
        #     print(f"3. Projected Feats NaN: {torch.isnan(video_features_projected).any()}")

        # # æ£€æŸ¥æ¨¡å‹æƒé‡æ˜¯å¦æœ‰ NaN
        # for name, param in self.named_parameters():
        #     if param.requires_grad and torch.isnan(param).any():
        #         print(f"âŒ Parameter {name} is NaN!")




        return super(VideoChatGPTLlamaModel, self).forward(
            input_ids=None, attention_mask=attention_mask, past_key_values=past_key_values,
            inputs_embeds=inputs_embeds, use_cache=use_cache,
            output_attentions=output_attentions, output_hidden_states=output_hidden_states,
            return_dict=return_dict
        )


class MultimodalVideoChatGPTLlamaForCausalLM(VideoChatGPTLlamaForCausalLM):
    """
    Multimodal Wrapper: è´Ÿè´£åˆå§‹åŒ– inner model å¹¶ä¼ é€’ forward å‚æ•°
    """
    config_class = VideoChatGPTConfig

    def __init__(self, config, pose_feature_dim: int = 68):
        # 1. åˆå§‹åŒ–çˆ¶ç±» (è¿™ä¼šåˆ›å»ºåŸå§‹çš„ self.model)
        super(LlamaForCausalLM, self).__init__(config)
        
        # 2. ã€å…³é”®ã€‘ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„å¤šæ¨¡æ€æ¨¡å‹æ›¿æ¢æ‰çˆ¶ç±»åˆ›å»ºçš„ self.model
        # è¿™æ · forward è°ƒç”¨æ—¶å°±ä¼šèµ°æˆ‘ä»¬ä¸Šé¢çš„ MultimodalVideoChatGPTLlamaModel
        self.model = MultimodalVideoChatGPTLlamaModel(
            config, 
            pose_feature_dim=pose_feature_dim
        )
        
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        video_spatio_temporal_features: Optional[torch.FloatTensor] = None,
        pose_spatio_temporal_features: Optional[torch.FloatTensor] = None, # æ¥æ”¶ Pose å‚æ•°
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # è°ƒç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„ self.model
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            video_spatio_temporal_features=video_spatio_temporal_features,
            pose_spatio_temporal_features=pose_spatio_temporal_features # ä¼ é€’ Pose å‚æ•°
        )

        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model/pipeline parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def prepare_inputs_for_generation(
        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs
    ):
        if past_key_values:
            input_ids = input_ids[:, -1:]

        if inputs_embeds is not None and past_key_values is None:
            model_inputs = {"inputs_embeds": inputs_embeds}
        else:
            model_inputs = {"input_ids": input_ids}

        # ç¡®ä¿ Pose å‚æ•°åœ¨ generate() æ—¶ä¹Ÿèƒ½è¢«ä¼ å…¥
        model_inputs.update({
            "past_key_values": past_key_values,
            "use_cache": kwargs.get("use_cache"),
            "attention_mask": attention_mask,
            "video_spatio_temporal_features": kwargs.get("video_spatio_temporal_features", None),
            "pose_spatio_temporal_features": kwargs.get("pose_spatio_temporal_features", None),
        })
        return model_inputs